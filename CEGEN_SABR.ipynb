{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d2c6be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 24 17:47:43 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 538.08                 Driver Version: 538.08       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 3500 Ada Gene...  WDDM  | 00000000:01:00.0 Off |                  Off |\n",
      "| N/A   40C    P3              19W / 102W |      0MiB / 12282MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9737f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "import statistics as stat\n",
    "import statsmodels.distributions as smd\n",
    "import os\n",
    "\n",
    "ML = 100000 #nombre d'echantillons al√©atoire\n",
    "t_init = 0\n",
    "T  = 0.25\n",
    "N1      = 10  # Compute N1 grid points\n",
    "dt1     = float(T - t_init) / N1 # Compute dt1\n",
    "ts1    = np.arange(t_init, T, dt1) # Compute grid on [0,T] with dt1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e51b835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sabr_trajectory(alpha, beta, rho, f0, sigma0, T, dt, n_sim):\n",
    "    #Function returning the trajectory couple (F, sigma)\n",
    "    np.random.seed(42)\n",
    "    n_steps = int(T / dt)\n",
    "    \n",
    "    F = np.zeros((n_sim, n_steps + 1))\n",
    "    sigma = np.zeros((n_sim, n_steps + 1))\n",
    "    \n",
    "    F[:, 0] = f0\n",
    "    sigma[:, 0] = sigma0\n",
    "    \n",
    "    dW1 = np.sqrt(dt) * np.random.randn(n_sim, n_steps) #Brownian motion for the forward dynamic \n",
    "    dW2 = rho * dW1 + np.sqrt(1 - rho**2) * np.sqrt(dt) * np.random.randn(n_sim, n_steps) #Browinan motion for the vol dynamic correlated to w1 by rho\n",
    "    \n",
    "    for t in range(n_steps):\n",
    "        F[:, t+1] = F[:, t] + sigma[:, t] * F[:, t]**beta * dW1[:, t]\n",
    "        sigma[:, t+1] = sigma[:, t] + alpha * sigma[:, t] * dW2[:, t]\n",
    "    \n",
    "    return F, sigma\n",
    "\n",
    "# Function that saves different models during the training phase\n",
    "def save_network(network, epoch_label, minibatch):\n",
    "    save_filename = 'net_{}_{}.pth'.format(epoch_label , minibatch)\n",
    "    save_path = os.path.join('./SavedModels102', save_filename)\n",
    "    torch.save(network.state_dict(), save_path)\n",
    "class Generator(torch.nn.Module):\n",
    "        def __init__(self, input_neurons, hidden_neurons, output_neurons ):\n",
    "            super(Generator, self).__init__()\n",
    "            self.hidden= nn.Linear(input_neurons, hidden_neurons)\n",
    "            self.hiddenM1= nn.Linear(hidden_neurons, hidden_neurons)\n",
    "            self.hiddenM2= nn.Linear(hidden_neurons, hidden_neurons)\n",
    "            #self.hiddenM3= nn.Linear(hidden_neurons, hidden_neurons)\n",
    "            self.Activ =torch.nn.Tanh()\n",
    "            #self.Activ =torch.nn.ReLU()\n",
    "\n",
    "            #self.Activ =torch.sin\n",
    "            self.eps = 1e-20\n",
    "            \n",
    "            self.bach1 = nn.BatchNorm1d(input_neurons)\n",
    "            self.bach2 = nn.BatchNorm1d(hidden_neurons)\n",
    "            self.bach3 = nn.BatchNorm1d(hidden_neurons)\n",
    "            self.bach4 = nn.BatchNorm1d(hidden_neurons)\n",
    "            self.bach5 = nn.BatchNorm1d(hidden_neurons)\n",
    "\n",
    "            self.out= nn.Linear(hidden_neurons, output_neurons)\n",
    "        def forward(self, x):\n",
    "            #x = self.bach1(x)\n",
    "            x = self.hidden(x)\n",
    "            x = self.Activ(x)\n",
    "            #x = self.bach2(x)           \n",
    "            x = self.hiddenM1(x)\n",
    "            x = self.Activ(x)\n",
    "            #x = self.bach3(x)           \n",
    "            x = self.hiddenM2(x)\n",
    "            x = self.Activ(x)\n",
    "            #x = self.bach4(x)           \n",
    "            #x = self.hiddenM3(x)            \n",
    "            #x = self.Activ(x)\n",
    "            #x = self.bach5(x)           \n",
    "            x = self.out(x)\n",
    "            \n",
    "            x[:,:-1] = torch.sigmoid(x[:,:-1])\n",
    "            x[:,-1] = torch.atan(x.clone()[:,-1])\n",
    "            return x\n",
    "\n",
    "\n",
    "def Wass2Dim(Cc1, Cc2, X,Y, Nm, b):\n",
    "    #Construction of the loss function on all the partition for a given time step\n",
    "    Res=0\n",
    "    for k in range(Nm):\n",
    "        if k==0:\n",
    "            n01=0\n",
    "            n02=0\n",
    "        else:\n",
    "            n01=n01+int(Cc1[k-1])\n",
    "            n02=n02+int(Cc2[k-1])\n",
    "        n1=int(Cc1[k])\n",
    "        n2=int(Cc2[k])\n",
    "        McX=torch.zeros(1)\n",
    "        McY=torch.zeros(1)\n",
    "        epsilon=10**(-16)\n",
    "        Lambda=100000\n",
    "        if (n1 != 0) and (n2!= 0):\n",
    "            if (min(Y[n02:n02+n2,0,0]) <= max(X[n01:n01+n1,0,0])) and (max(Y[n02:n02+n2,0,0]) >= min(X[n01:n01+n1,0,0])):\n",
    "                if (n1!=1) and (n2!=1):\n",
    "                    McX1=(1/n1)*torch.sum(X[n01:n01+n1,0,1]) #Monte Carlo Expectance of X dim 1\n",
    "                    McX2=(1/n1)*torch.sum(X[n01:n01+n1,1,1]) #Monte Carlo Expectance of X dim 2 \n",
    "\n",
    "                    McY1=(1/n2)*torch.sum(Y[n02:n02+n2,0,1]) #Monte Carlo Expectance of Y dim 1\n",
    "                    McY2=(1/n2)*torch.sum(Y[n02:n02+n2,1,1]) #Monte Carlo Expectance of Y dim 2\n",
    "\n",
    "                    #Compute the square root of  matrix CovX and CovY\n",
    "                    CovX=torch.cov(X[n01:n01+n1,:,1].T)\n",
    "                    CX=torch.linalg.eig(CovX) # Eigen values\n",
    "                    \n",
    "                    CEig=torch.view_as_real(CX[0])\n",
    "                    S=torch.view_as_real(CX[1])\n",
    "                    Si=torch.zeros(2,2)\n",
    "                    Si[:,0]=S[:,0][:,0]\n",
    "                    Si[:,1]=S[:,1][:,0]\n",
    "                    \n",
    "                    SqCovX=Si.mm(torch.diag((CEig[:,0])**(1/2))).mm(torch.transpose(Si, 0, 1))\n",
    "                    CovY=torch.cov(Y[n02:n02+n2,:,1].T)\n",
    "                    \n",
    "                    S14=(SqCovX.mm(CovY)).mm(SqCovX)\n",
    "                    C2X=torch.linalg.eig(S14)\n",
    "                    C2Eig=torch.view_as_real(C2X[0])\n",
    "                    S2=torch.view_as_real(C2X[1])\n",
    "                    S2i=torch.zeros(2,2)\n",
    "                    S2i[:,0]=S2[:,0][:,0]\n",
    "                    S2i[:,1]=S2[:,1][:,0]\n",
    "                    Sq4=S2i.mm(torch.diag((C2Eig[:,0])**(1/2))).mm(torch.transpose(S2i, 0, 1))\n",
    "                    if k==0:\n",
    "                        if (CovX[0,0] > epsilon) and (CovY[0,0] > epsilon):\n",
    "                            B=torch.trace(CovX)+torch.trace(CovY)-2*torch.trace(Sq4)\n",
    "                            Res=(McX1-McY1)**2+(McX2-McY2)**2+B\n",
    "                        else :\n",
    "                            Res=Lambda*(abs(McX1-McY1)+abs(McX2-McY2))\n",
    "                    else:\n",
    "                        if (CovX[0,0] > epsilon) and (CovY[0,0]  > epsilon):\n",
    "                            B=torch.trace(CovX)+torch.trace(CovY)-2*torch.trace(Sq4) #Bures distance\n",
    "                            Res=Res+(McX1-McY1)**2+(McX2-McY2)**2+B #Wasserstein 2 norm\n",
    "                        else :\n",
    "                            Res=Res+Lambda*(abs(McX1-McY1)+abs(McX2-McY2)) #Penalization term if CovX or CovY is near zero\n",
    "                else:\n",
    "                    if (n1==1) and (int(n01+n1)==int(b)):\n",
    "                        McX=(1/n1)*X[n01]\n",
    "                    else:\n",
    "                        McX=X[n01+n1]   \n",
    "                    if (n2!=1):\n",
    "                        McY=(1/n2)*torch.sum(Y[n02:n02+n2])\n",
    "                    elif (n2==1) and (int(n02+n2)==int(b)):\n",
    "                        McY=(1/n2)*Y[n02]\n",
    "                    else:\n",
    "                        McY=Y[n02+n2]\n",
    "                    if k==0:\n",
    "                        Res=Lambda*abs(McX-McY)\n",
    "                    else:\n",
    "                        Res=Res+Lambda*abs(McX-McY)\n",
    "\n",
    "    return Res      \n",
    "\n",
    "\n",
    "def PartitionQuantilesVect(V, Nm, b, d):\n",
    "    #PArtition function using quantile method\n",
    "    Qk=stat.quantiles((V[:,d,0]).detach(),n=Nm)\n",
    "    Ik=torch.zeros(Nm+1)\n",
    "    Ik[0]=min(V[:,d,0])\n",
    "    \n",
    "    Ik[Nm]=max(V[:,d,0])\n",
    "    for l in range(0,Nm-1):\n",
    "        Ik[l+1]=Qk[l]\n",
    "    \n",
    "    f=0\n",
    "    T=torch.zeros(b,2,3);\n",
    "    i=0\n",
    "    Cc=torch.zeros(Nm)\n",
    "\n",
    "    for p in range(Nm):\n",
    "        s=0\n",
    "        for l in range(b):\n",
    "            if (Ik[p] < V[l,d,0]) and (V[l,d,0]<= Ik[p+1] ):\n",
    "                T[f,0,:-1]=V[l,0,:]\n",
    "                T[f,1,:-1]=V[l,1,:]\n",
    "                f=f+1\n",
    "                s=s+1             \n",
    "                #print('f=',f,'l=',l,'p=',p)\n",
    "        if s!= 0:\n",
    "            T[f-1,-1:]=int(s)\n",
    "            Cc[i]=int(s)\n",
    "            i=i+1\n",
    "        #if Cc[0]==0:\n",
    "            #breakpoint()\n",
    "    return (T, Cc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a987a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''*********************Data set*********************'''\n",
    "\n",
    "# Data set for the SABR model\n",
    "\n",
    "alpha = 0.2 #alpha parameter\n",
    "beta = 0.8 #beta parameter\n",
    "rho = -0.3 #rho correlation parameter\n",
    "f0 = 100 #Forward initial value\n",
    "sigma0 = 0.36 #volatility initial value\n",
    "ML = 10000\n",
    "\n",
    "F_fine, sigma_fine = sabr_trajectory(alpha, beta, rho, f0, sigma0, T, dt1, ML) #SABR Trajectories generation\n",
    "\n",
    "StNumML1=np.zeros((ML, 2, ts1.size+1), dtype=np.float32) #Matrix of the input data \n",
    "StNumML1[:,0,:]=F_fine\n",
    "StNumML1[:,1,:]=sigma_fine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c9055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Networks'''\n",
    "\n",
    "#Generator Network\n",
    "NetworkG = Generator(input_neurons = 2, hidden_neurons = 16, output_neurons = 3)\n",
    "\n",
    "'''Optimizers'''\n",
    "\n",
    "#Generator optimizer Network\n",
    "optimizerG = torch.optim.Adam(NetworkG.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93fc3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batche_size=300\n",
    "\n",
    "Ntrain=ML  #Size of the training set\n",
    "NetworkG.train()\n",
    "LossMSE=nn.MSELoss()\n",
    "\n",
    "'''Partition'''\n",
    "\n",
    "for epoch in range(100):\n",
    "    for mini_batches in range(int(Ntrain/batche_size)):\n",
    "            #mini_batches=0\n",
    "            Nm=2\n",
    "            Xtrain=torch.as_tensor(StNumML1[mini_batches*batche_size:(mini_batches+1)*batche_size,:,:])\n",
    "            lik=0\n",
    "            Ypi=torch.empty(batche_size,2)\n",
    "            Ypp=torch.zeros(batche_size,2)\n",
    "            Ypi[:,:]=Xtrain[:,:,1]\n",
    "            l=0\n",
    "            for j in range(1,ts1.size):\n",
    "                YTorch=Xtrain[:,:,j:j+2]\n",
    "                ParaG=NetworkG(Ypi)\n",
    "                #print('ParaG=',ParaG)\n",
    "                galpha=ParaG[:,0] #Alpha predicted parameter\n",
    "                gbeta=ParaG[:,1] #beta predicted parameter\n",
    "                grho=ParaG[:,2] #rho predicted parameter\n",
    "                torch.random.seed()\n",
    "                dW   = torch.normal(0,1,size=(1, 2*batche_size))\n",
    "                dW21 = dW[0,:batche_size] #generation of the brownian increment for the Forward process\n",
    "                dW31 = grho*dW21+torch.sqrt(1 - grho**2) *dW[0,batche_size:] #generation of the brownian increment for the volatility process\n",
    "                \n",
    "                Ypp[:,0]=Ypi[:,0]+Ypi[:,1]*(Ypi[:,0]**gbeta)*(dt1**(1/2))*dW21[:] #Generation of The predicted forward process \n",
    "                Ypp[:,1]=Ypi[:,1]+(galpha)*Ypi[:,1]*(dt1**(1/2))*dW31[:] #Generation of The predicted volatility process\n",
    "                \n",
    "                Ypi=(Ypp).clone()\n",
    "                YpT=torch.empty(batche_size,2,2)\n",
    "                YpT[:,0,:]=torch.cat(( Ypi[:,0].view(-1,1), (Ypp[:,0]).view(-1,1)), dim=1)\n",
    "                YpT[:,1,:]=torch.cat(( Ypi[:,1].view(-1,1), (Ypp[:,1]).view(-1,1)), dim=1)\n",
    "\n",
    "                for d in range(2):\n",
    "                    Y, CcY=PartitionQuantilesVect(YTorch, Nm, batche_size, d) #Partition part using Quantile method\n",
    "                    Yp, CcYp=PartitionQuantilesVect(YpT, Nm, batche_size, d)\n",
    "                    \n",
    "                    if (j==1):\n",
    "                            lik=lik+Wass2Dim(CcY, CcYp, Y[:,:,:], (Yp[:,:,:]), Nm, batche_size) # Loss function on the partitions and over all time steps\n",
    "                           # breakpoint()\n",
    "                    else:\n",
    "                            lik=lik.clone()+Wass2Dim(CcY, CcYp, Y[:,:,:], (Yp[:,:,:]), Nm, batche_size) # Loss function on the partitions and over all time steps         \n",
    "            #print('MeanSigB=',torch.mean(ParaG[:,1]/Ypi[:,0]))\n",
    "            #print('MeanDriftB=',torch.mean(ParaG[:,0]/Ypi[:,0]))\n",
    "            Generator_loss = lik\n",
    "            optimizerG.zero_grad()\n",
    "            #Genrator network backpropagation\n",
    "            Generator_loss.backward(retain_graph=True)\n",
    "            #Genrator network parameters update\n",
    "            optimizerG.step()\n",
    "            #print('ParaG=',ParaG)\n",
    "            print('epoch=', epoch)\n",
    "            print('Error Generator=', Generator_loss/(Nm*d*N1))\n",
    "            #print('MeanSigA=',torch.mean(ParaG[:,1]/Ypi[:,0]))\n",
    "            #print('MeanDriftA=',torch.mean(ParaG[:,0]/Ypi[:,0]))\n",
    "            print('MeanBeta=',torch.mean(ParaG[:,1]))\n",
    "            print('MeanAlpha=',torch.mean(ParaG[:,0]))\n",
    "            print('grho=',torch.mean(ParaG[:,2]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
